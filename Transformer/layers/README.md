# Layers Directory

This directory contains the core building blocks of the Transformer architecture:

## Files
- `multi_head_attention.py`: Implementation of the Multi-Head Attention mechanism
  - Contains the `MultiHeadAttention` class
  - Handles query, key, value transformations
  - Implements scaled dot-product attention
  - Manages attention masking

## Purpose
The layers in this directory implement the fundamental components that make up the Transformer architecture. Each layer is designed to be modular and can be used independently or as part of the larger Transformer model. 