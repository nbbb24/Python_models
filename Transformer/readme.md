https://arxiv.org/pdf/1706.03762



https://medium.com/data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021

## Multi-Head attention
![Multi-Head Attention](Images\MHA.png)
![scale_dot_prodcut](Images\scale_dot_product.png)


##
Position Encoding
Feed-Forward Networks